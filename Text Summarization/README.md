# Text Summarization

- [Text Summarization](https://paperswithcode.com/task/text-summarization) on **PapersWithCode**
- []()

<img src="https://raw.githubusercontent.com/ElizaLo/NLP-Natural-Language-Processing/master/Text%20Summarization/img/ML7062-img1.jpg" width="644" height="233">

- You can apply text summarization to **identify key sentences within a document** or **identify key sentences across multiple documents**.
- Text summarization can produce two types of summaries: **extractive** and **abstractive**. 
  - **Extractive** summaries donâ€™t contain any machine-generated text and are a collection of important sentences selected from the input document. 
  - **Abstractive** summaries contain new human-readable phrases and sentences generated by the text summarization model. 
- Most text summarization systems are based on **extractive summarization** because accurate abstractive text summarization is difficult to achieve.

## Datasets

- [Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model](https://arxiv.org/pdf/1906.01749v3.pdf)
  - > While compressing information into a shorter text is the goal of summarization, this dataset tests the ability of abstractive models to generate fluent text concise in meaning while also coherent in the entirety of its generally longer output, which we consider an interesting challenge. 
- 

## Models

### BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - Model Overview

BART is a transformer encoder-encoder (seq2seq) model that combines an autoregressive (GPT-like) decoder with a bidirectional (BERT-like) encoder.

BART ( Bidirectional and Auto-Regressive) from transformers is a sequence-to-sequence model trained as a denoising autoencoder. This means that a fine-tuned BART model can take a text sequence (for example, English) as input and produce a different text sequence at the output (for example, French).

BART was trained as a denoising autoencoder, so the training data includesÂ _â€œcorruptedâ€_ Â orÂ _â€œnoisyâ€_Â  text, which would be mapped to clean or original text. So what exactly counts asÂ _â€œnoisyâ€_Â  for text data. The authors of BART settle on using some existing and some new noising techniques for pretraining. The noising schemes they use are Token Masking, Token Deletion, Text Infilling, Sentence Permutation, and Document Rotation. 

Looking into each of these transformations:

- **Token Masking:** Random tokens in a sentence are replaced with [MASK]. The model learns how to predict the single token based on the rest of the sequence.
- **Token Deletion:** Random tokens are deleted. The model must learn to predict the token content and find the position where the token was deleted from.
- **Text Infilling:** A fixed number of contiguous tokens are deleted and replaced with a single [MASK] token. The model must learn the content of the missing tokens and the number of tokens.
- **Sentence Permutation:** Sentences (separated by full stops) are permuted randomly. This helps the model to learn the logical entailment of sentences.
- **Document Rotation:** The document is rearranged to start with a random token. The content before the token is appended at the end of the document. This gives insights into how the document is typically arranged and how the beginning or ending of a document looks like.

However, not all transformations are employed in training the final BART model. Based on a comparative study of pre-training objectives, the authors use only text infilling and sentence permutation transformations, with about 30% of tokens being masked and all sentences permuted.

BARTÂ is the _**abstractive model**_ the same as other popular models for text summarization such asÂ **BertSum**Â andÂ **Pegasus**, which means summaries contain new human-readable phrases and sentences generated by the model.

For the summarization task we need to not only stack the inputs but also prepare the targets on the decoder side. BART is an encoder-decoder transformer and thus has the classicÂ seq2seqÂ architecture. In a `seq2seq` setup, a common approach is to apply _â€œteacher forcingâ€_ in the decoder. With this strategy, the decoder receives input tokens (like in decoder-only models such as GPT-2) that consists of the labels shifted by one in addition to the encoder output; so, when making the prediction for the next token the decoder gets the ground truth shifted by one as an input.

**How many Parameters does BART have?**

BART is constructed from a bi-directional encoder like in BERT and an autoregressive decoder like GPT. BERT has around 110M parameters while GPT has 117M, such trainable weights. BART being a sequenced version of the two, fittingly has nearly 140M parameters. Many parameters are justified by the supreme performance it yields on several tasks compared to fine-tuned BERT or its variations like RoBERTa, which has 125M parameters in its base model.

BART outperforms RoBERTa in several fine-tuning tasks.

- ðŸ“„ **Paper:** [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- **Implementations:**
  - ðŸ¤— **HuggingFace:** 
    - [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)
    - [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6)
      - > The model has been trained on two datasets: TheÂ [CNN Dailymail dataset](https://huggingface.co/datasets/cnn_dailymail) and theÂ [Extreme Summarization (XSum) dataset](https://huggingface.co/datasets/xsum). The numbers 12 and 6 in the model name refer to the number of encoder layers and decoder layers, respectively.
    - 
- **Articles:**
  - [Transformers BART Model Explained for Text Summarization](https://www.projectpro.io/article/transformers-bart-model-explained/553)

#### :bangbang: Issues

- [ ] The long text input
  - [Bart now enforces maximum sequence length in Summarization Pipeline #4224](https://github.com/huggingface/transformers/issues/4224)
  - [pipeline does not do truncation on long texts input, error message found #5983](https://github.com/huggingface/transformers/issues/5983)
  
    > We see that the articles can be very long compared to the target summary. **Long articles pose a challenge to most transformer models since the context size is usually limited to 1024 tokens or so**, which is equivalent to a few paragraphs of text. _The standard, yet crude way to deal with this for summarization is to simply truncate the texts beyond the modelâ€™s context size. Obviously there could be important information for the summary toward the end of the text, but for now we need to live with this limitation of the model architectures._

    > **How we can summarize documents where the texts are longer than the modelâ€™s context length?** _Unfortunately, there is no single strategy to solve this problem, and to date this is still an open and active research question._

    > Unfortunately, this problem also manifests when deploying BART on SageMaker via sagemaker.huggingface.HuggingFaceModel. When a request with **> 1024 tokens is sent**, the SageMaker endpoint crashes with an out-of-range CUDA error . What's worse, subsequent requests with smaller inputs fail with the same CUDA error. The only fix is to redeploy the endpoint.
  
    > For now, we're using an encode-truncate-decode workaround like below, but there clearly has to be a better way:
  
```python
# Inputs longer than 1024 tokens cause irrecoverable CUDA errors on
# SageMaker. Make sure that each text is at most 1024 tokens.
inputs = self.tokenizer(texts, max_length=1024, padding="longest",
                        truncation=True)
truncated_texts = [self.tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                   for i in inputs["input_ids"]]
output = predictor.predict({"inputs": truncated_texts, "parameters": parameters})
summaries = [summary["summary_text"] for summary in output]
```

**Possible solutions:**

- Padding and truncation

  - [Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation)
  - [pipeline parameters not read by the deployed endpoint](https://issuehint.com/issue/aws/sagemaker-huggingface-inference-toolkit/37)

## Long Text Summarization Methods

- [4 Powerful Long Text Summarization Methods With Real Examples](https://www.width.ai/post/4-long-text-summarization-methods)

The main con we see with long text summarization using **BertSum** is the underlying token limit of **BERT**. **BertSum** has an **_input token limit of 512_** which is much smaller than what we see today with **GPT-3** **_(4000+ in the newest instruct version)_**. This means for long text summarization we have to do a few special things:

1. Build chunking algorithms to split up the text.
2. Manage a new level of data variance considering each text chunk doesnâ€™t contain all contextual information from above. 
3. Manage combining chunk outputs at the end.


## Evaluation

- [SummEval: Re-evaluating Summarization Evaluation](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation) by **MIT Press Direct**


## Articles

- []()
