<img src="https://raw.githubusercontent.com/ElizaLo/NLP-Natural-Language-Processing/master/img/Evaluation_Metrics%20_in%20_NLP.png" width="1050" height="150"/>

| Title | Description, Information |
| :---:         |          :--- |
|[Perplexity](https://en.wikipedia.org/wiki/Perplexity)|<ul><p>In [information theory](https://en.wikipedia.org/wiki/Information_theory), perplexity is a measurement of how well a [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution) or [probability model](https://en.wikipedia.org/wiki/Probability_model) predicts a [sample](https://en.wikipedia.org/wiki/Sample_(statistics)). It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample.</p><p>In natural language processing, perplexity is a way of evaluating [language models](https://en.wikipedia.org/wiki/Language_model). A language model is a probability distribution over entire sentences or texts.</p><p> </p><p> </p><p> </p><p>**ðŸ“„ Articles:**</p><li>[Perplexity in Language Models](https://towardsdatascience.com/perplexity-in-language-models-87a196019a94)</li><li> ðŸ¤— [Perplexity of fixed-length models](https://huggingface.co/docs/transformers/perplexity) by Hugging Face</li><li>[]()</li></ul>|
|[Coherence (linguistics)](https://en.wikipedia.org/wiki/Coherence_(linguistics))|<ul><p>Coherence in linguistics is what makes a text [semantically](https://en.wikipedia.org/wiki/Semantics) meaningful. It is especially dealt with in [text linguistics](https://en.wikipedia.org/wiki/Text_linguistics). Coherence is achieved through syntactical features such as the use of [deictic](https://en.wikipedia.org/wiki/Deictic), [anaphoric](https://en.wikipedia.org/wiki/Anaphora_(linguistics)) and [cataphoric](https://en.wikipedia.org/wiki/Cataphora) elements or a logical tense structure, as well as [presuppositions](https://en.wikipedia.org/wiki/Presupposition) and [implications](https://en.wikipedia.org/wiki/Logical_consequence) connected to general world knowledge. The purely linguistic elements that make a text coherent are subsumed under the term [cohesion](https://en.wikipedia.org/wiki/Cohesion_(linguistics)).</p><p></p><p> </p><p></p><p> </p><p>**ðŸ“„ Articles:**</p><li>[When Coherence Score is Good or Bad in Topic Modeling?](https://www.baeldung.com/cs/topic-modeling-coherence-score)</li><li>[Understanding Topic Coherence Measures](https://towardsdatascience.com/understanding-topic-coherence-measures-4aa41339634c)</li><li>[]()</li><li>[]()</li></ul>|

# ðŸ’  Other possible metrics

## ðŸ”¹ Similarity

| Title | Description, Information |
| :---:         |          :--- |
|**Tanimoto Coefficient Similarity Method**|<ul><li> ðŸ“„ **Paper:** [Tanimoto coefficient based Word Sense Disambiguation](https://www.ijarnd.com/manuscripts/v4i7/V4I7-1141.pdf) </li><li> ðŸ“„ **Paper:** [Efficient identification of Tanimoto nearest neighbors](https://link.springer.com/article/10.1007/s41060-017-0064-z) </li></ul>|

## ðŸ“„ Atricles

- [The Most Common Evaluation Metrics In NLP](https://towardsdatascience.com/the-most-common-evaluation-metrics-in-nlp-ced6a763ac8b)
- [Top Evaluation Metrics For Your NLP Model](https://datascience.fm/top-evaluation-metrics-for-nlp-models/)
