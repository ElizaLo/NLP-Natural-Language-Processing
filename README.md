<img src="https://github.com/ElizaLo/NLP-Natural-Language-Processing/blob/master/img/Banner_NLP.png" width="900" height="100">

[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgithub.com%2FElizaLo%2FNLP-Natural-Language-Processing&count_bg=%23027A06&title_bg=%23A7A7B0&icon=python.svg&icon_color=%23E7E7E7&title=Repository+Views&edge_flat=false)](https://hits.seeyoufarm.com)

### Constantly updated. Subscribe not to miss anything.

> - [ ] For **Machine Learning**  algorithms please check [Machine Learning](https://github.com/ElizaLo/Machine-Learning) repository.

> - [ ] For **Deep Learning** algorithms please check [Deep Learning](https://github.com/ElizaLo/Deep-Learning) repository.

> - [ ] For **Computer Vision** please check [Computer Vision](https://github.com/ElizaLo/Computer-Vision) repository.

## üéì Courses 

  - [Natural Language Processing - Stanford University| Dan Jurafsky, Christopher](https://www.youtube.com/playlist?list=PLLssT5z_DsK8HbD2sPcUIDfQ7zmBarMYv)
  - [Natural Language Processing with Deep Learning (Stanford CS224N)](https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z) 
     - [Course website](http://web.stanford.edu/class/cs224n/)
     - Modern NLP techniques from recurrent neural networks and word embeddings to transformers and self-attention. Covers applied topics like questions answering and text generation.
  - [Natural Language Processing | University of Michigan](https://www.youtube.com/playlist?list=PLLssT5z_DsK8BdawOVCCaTCO99Ya58ryR)
  - [A Code-First Introduction to NLP course](https://github.com/fastai/course-nlp) by fast.ai
  - [From Languages to Information](https://web.stanford.edu/class/cs124/) by Stanford University
  - [Deep Learning for Natural Language Processing](https://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/) by University of Oxford
  - [Natural Language Processing](https://courses.cs.washington.edu/courses/cse517/17wi/) by University of Washington
  - [Natural Language Processing](https://github.com/yandexdataschool/nlp_course/tree/master) by Yandex Data School
  - [Natural Language Processing](https://www.coursera.org/learn/language-processing) by National Research University Higher School of Economics (via Coursera)
  - [Applied Natural Language Processing](http://people.ischool.berkeley.edu/~dbamman/info256.html) by UC Berkeley
  - [Advanced Methods in Natural Language Processing](https://www.cs.tau.ac.il/~joberant/teaching/nlp_spring_2019/index.html) by Tel Aviv University
  - Coursera:
    - [–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤, –Ω–∞–ø–∏—Å–∞–Ω–Ω—ã—Ö –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —è–∑—ã–∫–∞—Ö](https://www.coursera.org/specializations/natural-language-processing?utm_source=deeplearningai&utm_medium=institutions&utm_content=NLP_6/17_ppt#howItWorks)
    - [Text Mining](https://www.coursera.org/learn/text-mining) (–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞)
  - LinkedIn Learning:
    - [Advanced NLP with Python for Machine Learning](https://www.linkedin.com/learning/advanced-nlp-with-python-for-machine-learning)
      - Folder with [code](https://github.com/ElizaLo/NLP-Natural-Language-Processing/blob/master/Courses/LinkedIn%20Learning/Advanced%20NLP%20with%20Python%20for%20Machine%20Learning/Exercises.ipynb)
  - [–ú–∞—Ç–µ—Ä—ñ–∞–ª–∏ –¥–ª—è –∫—É—Ä—Å—É NLP –≤ –ü—Ä–æ–¥–∂–µ–∫—Ç–æ—Ä—ñ](https://github.com/sudodoki/prj-nlp/tree/master)
  - [Advaced NLP with spaCy](https://course.spacy.io/en/)
  

  
## Books

- [A lot of NLP books](https://www.dropbox.com/sh/b1c2ulwua9zy574/AACswS1E0IB9LdPDxQ6fexm4a?dl=0) (Natural Language Processing)
- [Natural Language Processing with Python ‚Äì Analyzing Text with the Natural Language Toolkit](http://www.nltk.org/book/), Steven Bird, Ewan Klein, and Edward Loper
- [Speech and Language Processing (3rd ed. draft)](https://web.stanford.edu/~jurafsky/slp3/), Dan Jurafsky and James H. Martin
- [Neural Network Methods for Natural Language Processing](http://www.morganclaypoolpublishers.com/catalog_Orig/samples/9781627052955_sample.pdf)

## YouTube 

- [Natural Language Processing (NLP) Zero to Hero](https://www.youtube.com/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S) by TensorFlow
- [Zero to Hero: NLP with Tensorflow and Keras (GDG Sofia meetup)](https://www.youtube.com/watch?v=ECRUJTKuKKs)

## Atricles

- [–ö–∞—Ä—å–µ—Ä–∞ –≤ IT: NLP Engineer –∏ NLP Researcher](https://dou.ua/lenta/articles/nlp-specialist/?from=tg)

## GitHub Repositories

- [ ] [Awesome NLP References](https://github.com/JudePark96/awesome-nlp-references), A curated list of resources dedicated to Knowledge Distillation, Recommendation System, especially Natural Language Processing (NLP)
- [ ] [NLP - Tutorial](https://github.com/makcedward/nlp)
- [ ] [Natural Language-Process Tutorials](https://github.com/skuchkula/Natural-Language-Processing-Tutorials)
- [ ] [NLP with Python](https://github.com/susanli2016/NLP-with-Python) - Scikit-Learn, NLTK, Spacy, Gensim, Textblob and more

### Deep Learning architectures for NLP

- [ ] [Introduction to Deep Learning for Natural Language Processing](https://github.com/rouseguy/europython2016_dl-nlp)
- [ ] [Deep Learning architectures for NLP](https://github.com/Tixierae/deep_learning_NLP) - Keras, PyTorch, and NumPy Implementations of Deep Learning Architectures for NLP

üîπ [100 Must-Read NLP Papers](http://masatohagiwara.net/100-nlp-papers/)

üîπ [Sci-Hub(Papers)](https://sci-hub.tw)

üîπ [Stanford, NLP Seminar Schedule](https://nlp.stanford.edu/seminar/)

üîπ [CS224n: Natural Language Processing with Deep Learning](http://web.stanford.edu/class/cs224n/)

üîπ [CIS 700-008 - Interactive Fiction and Text Generation](http://interactive-fiction-class.org/)

üîπ [Harvard NLP](http://nlp.seas.harvard.edu)

üîπ [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

## Useful NLP Libraries for Python 

- [ ] [Gensim](https://radimrehurek.com/gensim/index.html)

## üî∫ Projects:
  - [**_Spam Detection_**](https://github.com/ElizaLo/ML-with-Jupiter#spam-detection)
  - [**_Text Generator_**](https://github.com/ElizaLo/ML-with-Jupiter#text-generator)
  - **_Quora Insincere Questions Classification_**
  - [**_Question Answering System using BiDAF Model on SQuAD_**](https://github.com/ElizaLo/Question-Answering-based-on-SQuAD)

## Data Analysis for Natural Language Processing

- [Exploratory Data Analysis for Natural Language Processing: A Complete Guide to Python Tools](https://neptune.ai/blog/exploratory-data-analysis-natural-language-processing-tools?utm_source=linkedin&utm_medium=post-in-group&utm_campaign=blog-exploratory-data-analysis-natural-language-processing-tools&utm_content=3063585)


## üî∫ Embedings

üîπ [Everything about Embeddings](https://medium.com/@b.terryjack/nlp-everything-about-word-embeddings-9ea21f51ccfe)

- [TF-IDF Vectorizer](https://github.com/ElizaLo/NLP-Natural-Language-Processing/tree/master/Word%20Embedings/TF-IDF%20Vectorizer)

  1. ### **_Word Embedings_**
      - **One-hot encoding**
      - **Feature vectors**
      - **Cooccurrence vectors**
      - **Sparse Distributed Representations (SDRs)**
      ----------------
      - **Static Word Embeddings:**
        - **Continuous Bag-of-Words (CBOW)**
          - [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
        - **Skip-gram Model**
          - [Distributed Representations of Words and Phrases and their Compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
          - [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
        - **Word2Vec**
          - [Word2Vec Tutorial](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)
          - [Implementing Deep Learning Methods and Feature Engineering for Text Data: The Continuous Bag of Words (CBOW)](https://www.kdnuggets.com/2018/04/implementing-deep-learning-methods-feature-engineering-text-data-cbow.html)
        - **FastText**
          - [Bag of Tricks for Efficient Text Classification](https://arxiv.org/pdf/1607.01759.pdf)
          - [Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606.pdf)
          - [FASTTEXT.ZIP:COMPRESSING TEXT CLASSIFICATION MODELS](https://arxiv.org/pdf/1612.03651.pdf)
        - **GloVe: Global Vectors for Word Representation**
          - [GloVe: Global Vectors for Word Representation Article](https://nlp.stanford.edu/pubs/glove.pdf)
          - [GloVe](https://nlp.stanford.edu/projects/glove/)
       -------------------------
      - **Deep Neural Networks for Word Representations:** 
        - **Sequence to Sequence Model (Seq2Seq)**
          - [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)
          - [Order Matters: Sequence to sequence for sets](https://arxiv.org/pdf/1511.06391.pdf)
          - [A Comparison of Sequence-to-Sequence Models for Speech Recognition](https://www.isca-speech.org/archive/Interspeech_2017/pdfs/0233.PDF)
          - [Sequence to Sequence ‚Äì Video to Text](https://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf)
          - [Understanding Encoder-Decoder Sequence to Sequence Model](https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346)
      ----------------------------
      - **Contextualized (Dynamic) Word Embeddings (LM):**
         - CoVe (Contextualized Word-Embeddings)
         - CVT (Cross-View Training)
         - ELMO (Embeddings from Language Models)
            - [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
         - ULMFiT (Universal Language Model Fine-tuning)
         - BERT (Bidirectional Encoder Representations from Transformers)
            - [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805.pdf)
            - [FROM Pre-trained Word Embeddings TO Pre-trained Language Models ‚Äî Focus on BERT](https://medium.com/@adriensieg/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598)
         - GPT & GPT-2 (Generative Pre-Training)
         - Transformer XL (meaning extra long)
         - XLNet (Generalized Autoregressive Pre-training)
         - ENRIE (Enhanced Representation through kNowledge IntEgration)
         - (FlairEmbeddings (Contextual String Embeddings for Sequence Labelling))
---------------------------
  2. ### **_Sentence Embedings_**
      - [How to obtain Sentence Vectors?](https://medium.com/explorations-in-language-and-learning/how-to-obtain-sentence-vectors-2a6d88bd3c8b)
      - [Deep-learning-free Text and Sentence Embedding](https://www.offconvex.org/2018/06/17/textembeddings/)
-------------------------------------------------------      
  3. ### **_Text / Document Embedings_**
      - [Document Embedding Techniques](https://towardsdatascience.com/document-embedding-techniques-fed3e7a6a25d)

## üî∫ Models and Algorithms:      
  1. **_Levenshtein Distance_**
      - [Wikipedia](https://en.wikipedia.org/wiki/Levenshtein_distance)
      - [Levenshtein Distance Calculator](https://phiresky.github.io/levenshtein-demo/)
  2. **_Conditional Random Fields_**
      - [Overview of Conditional Random Fields](https://medium.com/ml2vec/overview-of-conditional-random-fields-68a2a20fa541)
  3. **_Formal Concept Analysis_**
      - [Formal Concept Analysis Examples](https://www.upriss.org.uk/fca/examples.html)
      - [FCA Online](https://fca-tools-bundle.com)
  4. **_Non-negative Matrix Factorization_**
      - [Wikipedia](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization)
      - **_Algorithms for Non-negative Matrix Factorization_**
        - [Algorithms for Non-negative Matrix Factorization](https://www.researchgate.net/publication/2538030_Algorithms_for_Non-negative_Matrix_Factorization) (Daniel D. **Lee** and H. Sebastian **Seung**) 
          - [Article at GitHub](https://github.com/ElizaLo/NLP/blob/master/Articles/Algorithms_for_Non-negative_Matrix_Factorization.pdf)
   5. **_Kullback‚ÄìLeibler divergence (relative entropy)_**
       - [Wikipedia](https://en.wikipedia.org/wiki/Kullback‚ÄìLeibler_divergence)
   6. **_Latent Semantic Analysis (LSA)_**
       - [Wikipedia](https://en.wikipedia.org/wiki/Latent_semantic_analysis)
       - **_Probabilistic Latent Semantic Analysis (PLSA)_**
          - [Wikipedia](https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis)
          - [–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π –ª–∞—Ç–µ–Ω—Ç–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑](http://www.machinelearning.ru/wiki/index.php?title=–í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω—ã–π_–ª–∞—Ç–µ–Ω—Ç–Ω—ã–π_—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π_–∞–Ω–∞–ª–∏–∑)
   7. **_Latent Dirichlet Allocation (LDA)_**
      - [Article](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) (David M. **Blei**, Andrew Y. **Ng**, Michael I. **Jordan**)
      - [–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ](http://www.machinelearning.ru/wiki/index.php?%20title=–¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ_–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ#.D0.9B.D0.B0.D1.82.D0.B5.%20D0.BD.D1.82.D0.BD.D0.BE.D0.B5_.D1.80.D0.B0.D0.B7.D0.BC.D0.%20B5.D1.89.D0.B5.D0.BD.D0.B8.D0.B5_.D0.94.D0.B8.D1.80.D0.B8.D1%20.85.D0.BB.D0.B5)
      - [Intuitive Guide to Latent Dirichlet Allocation](https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158)
   8. **_Gibbs sampling_**
      - [Wikipedia](https://en.wikipedia.org/wiki/Gibbs_sampling)
      - [Topic modeling using Latent Dirichlet Allocation(LDA) and Gibbs Sampling explained!](https://medium.com/analytics-vidhya/topic-modeling-using-lda-and-gibbs-sampling-explained-49d49b3d1045)
   9. 
-----------------------------------------------------------------------

## Knowlrdge Graphs

- [ON UNDERSTANDING KNOWLEDGE GRAPH REPRESENTATION](https://arxiv.org/pdf/1909.11611.pdf)

## Coreference

# Question Answering System using BiDAF Model on SQuAD

Implemented a Bidirectional Attention Flow neural network as a baseline on SQuAD, improving Chris Chute's model [implementation](https://github.com/chrischute/squad/blob/master/layers.py), adding word-character inputs as described in the original paper and improving [GauthierDmns' code](https://github.com/GauthierDmn/question_answering).

  - [Project Repository](https://github.com/ElizaLo/Question-Answering-based-on-SQuAD)
  - [Paper](https://github.com/ElizaLo/NLP/blob/master/Question%20Answering%20System/Question%20Answering%20System%20based%20on%20SQuAD.pdf)
  - [Used Articles](https://github.com/ElizaLo/NLP/tree/master/Question%20Answering%20System/Articles)
  - [Useful Articles](https://github.com/ElizaLo/Question-Answering-based-on-SQuAD#useful-articles)
  - [Useful Links](https://github.com/ElizaLo/Question-Answering-based-on-SQuAD#useful-links)
  
  
  # Useful Articles
  
  - [ ] [Interpretation of Natural Language Rules in Conversational Machine Reading](https://arxiv.org/abs/1809.01494)
  - [ ] [Skip-Thought Vectors](https://github.com/ElizaLo/NLP/blob/master/University%20Course%20of%20NLP/Articles/5950-skip-thought-vectors.pdf), [Article](https://arxiv.org/abs/1506.06726)
  
 # Some Concepts
 
 - [x] **Selectional Preference** - (Katz and Fodor, 1963; Wilks, 1975; Resnik, 1993) are the tendency for a word to semantically select or constrain which other words may appear in a direct syntactic relation with it." In case this selection is expressed in binary term (**_allowed/not-allowed_**), it is also called selectional restriction (S√©aghdha and Korhonen, 2014). SP can be contrasted with **_verb subcategorization_** "with subcategorization describing the syntactic arguments taken by a verb, and selectional preferences describing the semantic preferences verbs have for their arguments" (Van de Cruys et al., 2012)
    - [Selectional preference, Natural Language Understanding Wiki](https://natural-language-understanding.fandom.com/wiki/Selectional_preference)
 - [x] **Selectional Restrictions** - 
    - [Selectional Restrictions, Jurafsky](https://web.stanford.edu/~jurafsky/slp3/slides/22_select.pdf)
