# Text Summarization

- [Text Summarization](https://paperswithcode.com/task/text-summarization) on **PapersWithCode**
- []()

----
<img src="https://raw.githubusercontent.com/ElizaLo/NLP-Natural-Language-Processing/master/Text%20Summarization/img/ML7062-img1.jpg" width="644" height="233">

- You can apply text summarization to **identify key sentences within a document** or **identify key sentences across multiple documents**.
- Text summarization can produce two types of summaries: **extractive** and **abstractive**. 
  - **Extractive** summaries don’t contain any machine-generated text and are a collection of important sentences selected from the input document. 
  - **Abstractive** summaries contain new human-readable phrases and sentences generated by the text summarization model. Traditional extractive models develop heuristics for scoring textual components by employing surface features (e.g., term frequency, text position, and critical keywords) as well as semantic relationships (e.g., discourse trees) among components in the document. [TextRank](https://www.aclweb.org/anthology/W04-3252/) is a case in point. Alternatively, extractive summarization can be formulated as a binary classification problem in which hand-crafted features are combined to predict whether a fragment should be included in the summary.
  - **Hybrid models:** adopt a two-stage procedure of content selection and paraphrasing. Extractors are employed in content selection to identify important fragments in source documents, which further influence abstractors in generating summaries.

- Most text summarization systems are based on **extractive summarization** because accurate abstractive text summarization is difficult to achieve.

## Datasets

- [Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model](https://arxiv.org/pdf/1906.01749v3.pdf)
  - > While compressing information into a shorter text is the goal of summarization, this dataset tests the ability of abstractive models to generate fluent text concise in meaning while also coherent in the entirety of its generally longer output, which we consider an interesting challenge. 
- 

## Models

| Title | Description, Information |
| :---:         |          :--- |
| **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - Model Overview** | 📄 **Paper:** [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461) |

### BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - Model Overview

BART is a transformer encoder-encoder (seq2seq) model that combines an autoregressive (GPT-like) decoder with a bidirectional (BERT-like) encoder.

BART ( Bidirectional and Auto-Regressive) from transformers is a sequence-to-sequence model trained as a denoising autoencoder. This means that a fine-tuned BART model can take a text sequence (for example, English) as input and produce a different text sequence at the output (for example, French).

BART was trained as a denoising autoencoder, so the training data includes _“corrupted”_  or _“noisy”_  text, which would be mapped to clean or original text. So what exactly counts as _“noisy”_  for text data. The authors of BART settle on using some existing and some new noising techniques for pretraining. The noising schemes they use are Token Masking, Token Deletion, Text Infilling, Sentence Permutation, and Document Rotation. 

Looking into each of these transformations:

- **Token Masking:** Random tokens in a sentence are replaced with [MASK]. The model learns how to predict the single token based on the rest of the sequence.
- **Token Deletion:** Random tokens are deleted. The model must learn to predict the token content and find the position where the token was deleted from.
- **Text Infilling:** A fixed number of contiguous tokens are deleted and replaced with a single [MASK] token. The model must learn the content of the missing tokens and the number of tokens.
- **Sentence Permutation:** Sentences (separated by full stops) are permuted randomly. This helps the model to learn the logical entailment of sentences.
- **Document Rotation:** The document is rearranged to start with a random token. The content before the token is appended at the end of the document. This gives insights into how the document is typically arranged and how the beginning or ending of a document looks like.

However, not all transformations are employed in training the final BART model. Based on a comparative study of pre-training objectives, the authors use only text infilling and sentence permutation transformations, with about 30% of tokens being masked and all sentences permuted.

BART is the _**abstractive model**_ the same as other popular models for text summarization such as **Pegasus**, which means summaries contain new human-readable phrases and sentences generated by the model.

For the summarization task we need to not only stack the inputs but also prepare the targets on the decoder side. BART is an encoder-decoder transformer and thus has the classic seq2seq architecture. In a `seq2seq` setup, a common approach is to apply _“teacher forcing”_ in the decoder. With this strategy, the decoder receives input tokens (like in decoder-only models such as GPT-2) that consists of the labels shifted by one in addition to the encoder output; so, when making the prediction for the next token the decoder gets the ground truth shifted by one as an input.

**How many Parameters does BART have?**

BART is constructed from a bi-directional encoder like in BERT and an autoregressive decoder like GPT. BERT has around 110M parameters while GPT has 117M, such trainable weights. BART being a sequenced version of the two, fittingly has nearly 140M parameters. Many parameters are justified by the supreme performance it yields on several tasks compared to fine-tuned BERT or its variations like RoBERTa, which has 125M parameters in its base model.

BART outperforms RoBERTa in several fine-tuning tasks.

- 📄 **Paper:** [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- **Implementations:**
  - 🤗 **HuggingFace:** 
    - [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)
    - [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6)
      - > The model has been trained on two datasets: The [CNN Dailymail dataset](https://huggingface.co/datasets/cnn_dailymail) and the [Extreme Summarization (XSum) dataset](https://huggingface.co/datasets/xsum). The numbers 12 and 6 in the model name refer to the number of encoder layers and decoder layers, respectively.
    - 
- **Articles:**
  - [Transformers BART Model Explained for Text Summarization](https://www.projectpro.io/article/transformers-bart-model-explained/553)

#### :bangbang: Issues

- [ ] The long text input
  - [Bart now enforces maximum sequence length in Summarization Pipeline #4224](https://github.com/huggingface/transformers/issues/4224)
  - [pipeline does not do truncation on long texts input, error message found #5983](https://github.com/huggingface/transformers/issues/5983)
  
    > We see that the articles can be very long compared to the target summary. **Long articles pose a challenge to most transformer models since the context size is usually limited to 1024 tokens or so**, which is equivalent to a few paragraphs of text. _The standard, yet crude way to deal with this for summarization is to simply truncate the texts beyond the model’s context size. Obviously there could be important information for the summary toward the end of the text, but for now we need to live with this limitation of the model architectures._

    > **How we can summarize documents where the texts are longer than the model’s context length?** _Unfortunately, there is no single strategy to solve this problem, and to date this is still an open and active research question._

    > Unfortunately, this problem also manifests when deploying BART on SageMaker via sagemaker.huggingface.HuggingFaceModel. When a request with **> 1024 tokens is sent**, the SageMaker endpoint crashes with an out-of-range CUDA error . What's worse, subsequent requests with smaller inputs fail with the same CUDA error. The only fix is to redeploy the endpoint.
  
    > For now, we're using an encode-truncate-decode workaround like below, but there clearly has to be a better way:
  
```python
# Inputs longer than 1024 tokens cause irrecoverable CUDA errors on
# SageMaker. Make sure that each text is at most 1024 tokens.
inputs = self.tokenizer(texts, max_length=1024, padding="longest",
                        truncation=True)
truncated_texts = [self.tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                   for i in inputs["input_ids"]]
output = predictor.predict({"inputs": truncated_texts, "parameters": parameters})
summaries = [summary["summary_text"] for summary in output]
```

**Possible solutions:**

- Padding and truncation

  - [Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation)
  - [pipeline parameters not read by the deployed endpoint](https://issuehint.com/issue/aws/sagemaker-huggingface-inference-toolkit/37)

## Long Text Summarization

Long documents introduce new problems to the process of summarization:

- **More noise:** It is safe to say that the number of main points within a 5000-word document hardly ever exceeds 10. That is to say, most parts in the document are barely expansions of some central ideas and thus, should be ignored.
- **Scattered main points:** Although there are a few of them, main points are widely scattered over the text, which makes full-text scan inevitable to extract all important information.
- **More resources needed:** As the text gets longer, we need a higher dimensional vector to encode it before feeding into neural models.

Four strategies are proposed to address the issues with long documents:

- **Truncating input to a fixed length.** This is the most straightforward approach but often the least efficient as it discards text which may contain important information, given main points are widely scattered over the text.
- **Focus on informative parts of the document.** We hypothesize that main points should be mentioned in some “main” sections in the document. By that, we only need to perform summarization on the subset of the original text.
  - >  **Shortcomings:** This is basically a heuristic method and cannot scale well if there are no bias sections, for example, in summarizing a novel.
- **Hybrid models.** Extracting only important portions of the text greatly reduces the problem space. This is a promising idea since we can take advantage of state-of-the-art results in both extractive and abstractive text summarization.
  - > **Shortcomings:** Since neural networks are mostly employed in state-of-the-art extractive summarization models, the problem of resources goes full circle.
- **Divide-and-conquer.** In each base case, we try to address the problem of summarizing a text portion within the original document. Then, partial summaries are combined in some way to create the final summary. This method possesses many properties to help us deal with mentioned issues of long documents: (1) by dividing a long text into manageable chunks, we can fully encode them within the constraint of resources; (2) subproblems are independently solved, which makes it easier for parallelism and full-text scan; (3) last but not least, the state-of-the-art results in the field do not go to waste.
  - > **Shortcomings:** Currently, each type of document requires a different dividing strategy. How to efficiently divide an arbitrary text into appropriate sections with relatively short length remains an open question that lacks general answers.


- [An Empirical Survey on Long Document Summarization](https://github.com/huankoh/long-doc-summarization)
- [4 Powerful Long Text Summarization Methods With Real Examples](https://www.width.ai/post/4-long-text-summarization-methods)


The main con we see with long text summarization using **BertSum** is the underlying token limit of **BERT**. **BertSum** has an **_input token limit of 512_** which is much smaller than what we see today with **GPT-3** **_(4000+ in the newest instruct version)_**. This means for long text summarization we have to do a few special things:

1. Build chunking algorithms to split up the text.
2. Manage a new level of data variance considering each text chunk doesn’t contain all contextual information from above. 
3. Manage combining chunk outputs at the end.

### Models

| Title | Description, Information |
| :---:         |          :--- |
| **BertSum** | 📄 **Paper:** [Fine-tune BERT for Extractive Summarization](https://arxiv.org/abs/1903.10318) - **Extractive Summarization**, [Evaluating Extractive Text Summarization with BERTSUM](https://web.stanford.edu/class/cs224n/reports/final_reports/report042.pdf) |
| **Recursively Summarizing Books with Human Feedback** | 📄 **Paper:** [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862)|
|**Longformer Summarization: Long-Document Transformer** | 📄 **Paper:** [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150v2), [Papers with Code - Longformer: The Long-Document Transformer](https://paperswithcode.com/paper/longformer-the-long-document-transformer#code), [HuggingFace - Longformer](https://huggingface.co/docs/transformers/model_doc/longformer), [Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens ](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb#scrollTo=W7-QHmRiAMB9) |
| **GPT-3** | 📄 **Paper:** [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165v4), [News Summarization and Evaluation in the Era of GPT-3](https://arxiv.org/pdf/2209.12356.pdf), [Papers with Code - GPT-3 Explained](https://paperswithcode.com/method/gpt-3)|
| **DANCER** | 📄 **Paper:** [A Divide-and-Conquer Approach to the Summarization of Long Documents](https://arxiv.org/abs/2004.06190)|

- [Simple Application to summarize data using GPT-3 Openai model](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/simple-application-to-summarize-data-using-gpt-3-openai-model/ba-p/3047978)
- [GPT3-text-summarization](https://github.com/juan-csv/GPT3-text-summarization)
- [Awesome GPT-3](https://github.com/elyase/awesome-gpt3)

## Evaluation

- [SummEval: Re-evaluating Summarization Evaluation](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation) by **MIT Press Direct**
- [NLP-progress - Summarization](http://nlpprogress.com/english/summarization.html) - Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.

| Title | Description, Information |
| :---:         |          :--- |
| **ROUGE: A Package for Automatic Evaluation of Summaries** | 📄 **Paper:** [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013/)|

**ROUGE: A Package for Automatic Evaluation of Summaries**

ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, measures the similarity of two texts by computing n-gram or word sequence overlaps on the scale from 0 to 100. Three variants of ROUGE which are commonly used in summarization includes:

- ROUGE-1: refers to the overlap of unigram (each word) between the two
- ROUGE-2: refers to the overlap of bigram (each two consecutive words) between the two
- ROUGE-L: refers to the longest common sequence between the two



## Articles

- [Bootcamp Tech Blog #4: Long Document Summarization](https://cinnamonai.medium.com/bootcamp-tech-blog-4-long-document-summarization-6bc25e3add94)
