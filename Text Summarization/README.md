# Text Summarization

- [Text Summarization](https://paperswithcode.com/task/text-summarization) on **PapersWithCode**
- []()

<img src="https://raw.githubusercontent.com/ElizaLo/NLP-Natural-Language-Processing/master/Text%20Summarization/img/ML7062-img1.jpg" width="644" height="233">

- You can apply text summarization to **identify key sentences within a document** or **identify key sentences across multiple documents**.
- Text summarization can produce two types of summaries: **extractive** and **abstractive**. 
  - **Extractive** summaries donâ€™t contain any machine-generated text and are a collection of important sentences selected from the input document. 
  - **Abstractive** summaries contain new human-readable phrases and sentences generated by the text summarization model. 
- Most text summarization systems are based on **extractive summarization** because accurate abstractive text summarization is difficult to achieve.

## Models

### BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - Model Overview

#### :bangbang: Issues

- [ ] The long text input
  - [Bart now enforces maximum sequence length in Summarization Pipeline #4224](https://github.com/huggingface/transformers/issues/4224)
  - [pipeline does not do truncation on long texts input, error message found #5983](https://github.com/huggingface/transformers/issues/5983)
  
  
  > Unfortunately, this problem also manifests when deploying BART on SageMaker via sagemaker.huggingface.HuggingFaceModel. When a request with **> 1024 tokens is sent**, the SageMaker endpoint crashes with an out-of-range CUDA error . What's worse, subsequent requests with smaller inputs fail with the same CUDA error. The only fix is to redeploy the endpoint.
  
  > For now, we're using an encode-truncate-decode workaround like below, but there clearly has to be a better way:
  
```python
# Inputs longer than 1024 tokens cause irrecoverable CUDA errors on
# SageMaker. Make sure that each text is at most 1024 tokens.
inputs = self.tokenizer(texts, max_length=1024, padding="longest",
                        truncation=True)
truncated_texts = [self.tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                   for i in inputs["input_ids"]]
output = predictor.predict({"inputs": truncated_texts, "parameters": parameters})
summaries = [summary["summary_text"] for summary in output]
```

**Possible solutions:**

- Padding and truncation

  - [Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation)
  - [pipeline parameters not read by the deployed endpoint](https://issuehint.com/issue/aws/sagemaker-huggingface-inference-toolkit/37)


## Evaluation

- [SummEval: Re-evaluating Summarization Evaluation](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation) by **MIT Press Direct**
