# Text Summarization

- [Text Summarization](https://paperswithcode.com/task/text-summarization) on **PapersWithCode**
- []()

----
<img src="https://raw.githubusercontent.com/ElizaLo/NLP-Natural-Language-Processing/master/Text%20Summarization/img/ML7062-img1.jpg" width="644" height="233">

- You can apply text summarization to **identify key sentences within a document** or **identify key sentences across multiple documents**.
- Text summarization can produce two types of summaries: **extractive** and **abstractive**. 
  - **Extractive** summaries don‚Äôt contain any machine-generated text and are a collection of important sentences selected from the input document. 
  - **Abstractive** summaries contain new human-readable phrases and sentences generated by the text summarization model. Traditional extractive models develop heuristics for scoring textual components by employing surface features (e.g., term frequency, text position, and critical keywords) as well as semantic relationships (e.g., discourse trees) among components in the document. [TextRank](https://www.aclweb.org/anthology/W04-3252/) is a case in point. Alternatively, extractive summarization can be formulated as a binary classification problem in which hand-crafted features are combined to predict whether a fragment should be included in the summary.
  - **Hybrid models:** adopt a two-stage procedure of content selection and paraphrasing. Extractors are employed in content selection to identify important fragments in source documents, which further influence abstractors in generating summaries.

- Most text summarization systems are based on **extractive summarization** because accurate abstractive text summarization is difficult to achieve.

## Datasets

- [Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model](https://arxiv.org/pdf/1906.01749v3.pdf)
  - > While compressing information into a shorter text is the goal of summarization, this dataset tests the ability of abstractive models to generate fluent text concise in meaning while also coherent in the entirety of its generally longer output, which we consider an interesting challenge. 
- 

## Models

| Title | Description, Information |
| :---:         |          :--- |
| **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - Model Overview** | üìÑ **Paper:** [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461), **abstractive** summarization |
| **SBERT: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks** | üìÑ **Paper:** [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084), **extractive** summarization |

### Extractive summarization

### SBERT: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

 In SBERT you can create a summary with the ability to say how many sentences you want. 

- üìÑ **Paper:** [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
- **Implementations:**
  - [SentenceTransformers Documentation](https://www.sbert.net/index.html)
  - [bert-extractive-summarizer](https://pypi.org/project/bert-extractive-summarizer/)
  - ü§ó **HuggingFace:** 
    - [sentence-transformers](https://huggingface.co/models?library=sentence-transformers&sort=downloads)

### Abstractive summarization

### BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - Model Overview

BART is a transformer encoder-encoder (seq2seq) model that combines an autoregressive (GPT-like) decoder with a bidirectional (BERT-like) encoder.

BART ( Bidirectional and Auto-Regressive) from transformers is a sequence-to-sequence model trained as a denoising autoencoder. This means that a fine-tuned BART model can take a text sequence (for example, English) as input and produce a different text sequence at the output (for example, French).

BART was trained as a denoising autoencoder, so the training data includes¬†_‚Äúcorrupted‚Äù_ ¬†or¬†_‚Äúnoisy‚Äù_¬† text, which would be mapped to clean or original text. So what exactly counts as¬†_‚Äúnoisy‚Äù_¬† for text data. The authors of BART settle on using some existing and some new noising techniques for pretraining. The noising schemes they use are Token Masking, Token Deletion, Text Infilling, Sentence Permutation, and Document Rotation. 

Looking into each of these transformations:

- **Token Masking:** Random tokens in a sentence are replaced with [MASK]. The model learns how to predict the single token based on the rest of the sequence.
- **Token Deletion:** Random tokens are deleted. The model must learn to predict the token content and find the position where the token was deleted from.
- **Text Infilling:** A fixed number of contiguous tokens are deleted and replaced with a single [MASK] token. The model must learn the content of the missing tokens and the number of tokens.
- **Sentence Permutation:** Sentences (separated by full stops) are permuted randomly. This helps the model to learn the logical entailment of sentences.
- **Document Rotation:** The document is rearranged to start with a random token. The content before the token is appended at the end of the document. This gives insights into how the document is typically arranged and how the beginning or ending of a document looks like.

However, not all transformations are employed in training the final BART model. Based on a comparative study of pre-training objectives, the authors use only text infilling and sentence permutation transformations, with about 30% of tokens being masked and all sentences permuted.

BART¬†is the _**abstractive model**_ the same as other popular models for text summarization such as¬†**Pegasus**, which means summaries contain new human-readable phrases and sentences generated by the model.

For the summarization task we need to not only stack the inputs but also prepare the targets on the decoder side. BART is an encoder-decoder transformer and thus has the classic¬†seq2seq¬†architecture. In a `seq2seq` setup, a common approach is to apply _‚Äúteacher forcing‚Äù_ in the decoder. With this strategy, the decoder receives input tokens (like in decoder-only models such as GPT-2) that consists of the labels shifted by one in addition to the encoder output; so, when making the prediction for the next token the decoder gets the ground truth shifted by one as an input.

**How many Parameters does BART have?**

BART is constructed from a bi-directional encoder like in BERT and an autoregressive decoder like GPT. BERT has around 110M parameters while GPT has 117M, such trainable weights. BART being a sequenced version of the two, fittingly has nearly 140M parameters. Many parameters are justified by the supreme performance it yields on several tasks compared to fine-tuned BERT or its variations like RoBERTa, which has 125M parameters in its base model.

BART outperforms RoBERTa in several fine-tuning tasks.

- üìÑ **Paper:** [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- **Implementations:**
  - ü§ó **HuggingFace:** 
    - [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)
    - [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6)
      - üìÑ **Paper:** [Pre-trained Summarization Distillation](https://arxiv.org/abs/2010.13002)
      - > The model has been trained on two datasets: The¬†[CNN Dailymail dataset](https://huggingface.co/datasets/cnn_dailymail) and the¬†[Extreme Summarization (XSum) dataset](https://huggingface.co/datasets/xsum). The numbers 12 and 6 in the model name refer to the number of encoder layers and decoder layers, respectively.
    - 
-  üì∞ **Articles:**
  - [Transformers BART Model Explained for Text Summarization](https://www.projectpro.io/article/transformers-bart-model-explained/553)

#### :bangbang: Issues

- [ ] The long text input
  - [Bart now enforces maximum sequence length in Summarization Pipeline #4224](https://github.com/huggingface/transformers/issues/4224)
  - [pipeline does not do truncation on long texts input, error message found #5983](https://github.com/huggingface/transformers/issues/5983)
  
    > We see that the articles can be very long compared to the target summary. **Long articles pose a challenge to most transformer models since the context size is usually limited to 1024 tokens or so**, which is equivalent to a few paragraphs of text. _The standard, yet crude way to deal with this for summarization is to simply truncate the texts beyond the model‚Äôs context size. Obviously there could be important information for the summary toward the end of the text, but for now we need to live with this limitation of the model architectures._

    > **How we can summarize documents where the texts are longer than the model‚Äôs context length?** _Unfortunately, there is no single strategy to solve this problem, and to date this is still an open and active research question._

    > Unfortunately, this problem also manifests when deploying BART on SageMaker via sagemaker.huggingface.HuggingFaceModel. When a request with **> 1024 tokens is sent**, the SageMaker endpoint crashes with an out-of-range CUDA error . What's worse, subsequent requests with smaller inputs fail with the same CUDA error. The only fix is to redeploy the endpoint.
  
    > For now, we're using an encode-truncate-decode workaround like below, but there clearly has to be a better way:
  
```python
# Inputs longer than 1024 tokens cause irrecoverable CUDA errors on
# SageMaker. Make sure that each text is at most 1024 tokens.
inputs = self.tokenizer(texts, max_length=1024, padding="longest",
                        truncation=True)
truncated_texts = [self.tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                   for i in inputs["input_ids"]]
output = predictor.predict({"inputs": truncated_texts, "parameters": parameters})
summaries = [summary["summary_text"] for summary in output]
```

**Possible solutions:**

- Padding and truncation

  - [Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation)
  - [pipeline parameters not read by the deployed endpoint](https://issuehint.com/issue/aws/sagemaker-huggingface-inference-toolkit/37)

## Long Text Summarization

Long documents introduce new problems to the process of summarization:

- **More noise:** It is safe to say that the number of main points within a 5000-word document hardly ever exceeds 10. That is to say, most parts in the document are barely expansions of some central ideas and thus, should be ignored.
- **Scattered main points:** Although there are a few of them, main points are widely scattered over the text, which makes full-text scan inevitable to extract all important information.
- **More resources needed:** As the text gets longer, we need a higher dimensional vector to encode it before feeding into neural models.

Four strategies are proposed to address the issues with long documents:

- **Truncating input to a fixed length.** This is the most straightforward approach but often the least efficient as it discards text which may contain important information, given main points are widely scattered over the text.
- **Focus on informative parts of the document.** We hypothesize that main points should be mentioned in some ‚Äúmain‚Äù sections in the document. By that, we only need to perform summarization on the subset of the original text.
  - >  **Shortcomings:** This is basically a heuristic method and cannot scale well if there are no bias sections, for example, in summarizing a novel.
- **Hybrid models.** Extracting only important portions of the text greatly reduces the problem space. This is a promising idea since we can take advantage of state-of-the-art results in both extractive and abstractive text summarization.
  - > **Shortcomings:** Since neural networks are mostly employed in state-of-the-art extractive summarization models, the problem of resources goes full circle.
- **Divide-and-conquer.** In each base case, we try to address the problem of summarizing a text portion within the original document. Then, partial summaries are combined in some way to create the final summary. This method possesses many properties to help us deal with mentioned issues of long documents: (1) by dividing a long text into manageable chunks, we can fully encode them within the constraint of resources; (2) subproblems are independently solved, which makes it easier for parallelism and full-text scan; (3) last but not least, the state-of-the-art results in the field do not go to waste.
  - > **Shortcomings:** Currently, each type of document requires a different dividing strategy. How to efficiently divide an arbitrary text into appropriate sections with relatively short length remains an open question that lacks general answers.


- [An Empirical Survey on Long Document Summarization](https://github.com/huankoh/long-doc-summarization)
- [4 Powerful Long Text Summarization Methods With Real Examples](https://www.width.ai/post/4-long-text-summarization-methods)


The main con we see with long text summarization using **BertSum** is the underlying token limit of **BERT**. **BertSum** has an **_input token limit of 512_** which is much smaller than what we see today with **GPT-3** **_(4000+ in the newest instruct version)_**. This means for long text summarization we have to do a few special things:

1. Build chunking algorithms to split up the text.
2. Manage a new level of data variance considering each text chunk doesn‚Äôt contain all contextual information from above. 
3. Manage combining chunk outputs at the end.

### Models

| Title | Description, Information |
| :---:         |          :--- |
| **BertSum** | üìÑ **Paper:** [Fine-tune BERT for Extractive Summarization](https://arxiv.org/abs/1903.10318) - **Extractive Summarization**, [Evaluating Extractive Text Summarization with BERTSUM](https://web.stanford.edu/class/cs224n/reports/final_reports/report042.pdf) |
| **Recursively Summarizing Books with Human Feedback** | üìÑ **Paper:** [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862)|
|**Longformer Summarization: Long-Document Transformer** | üìÑ **Paper:** [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150v2), [Papers with Code - Longformer: The Long-Document Transformer](https://paperswithcode.com/paper/longformer-the-long-document-transformer#code), [HuggingFace - Longformer](https://huggingface.co/docs/transformers/model_doc/longformer), [Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens ](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb#scrollTo=W7-QHmRiAMB9) |
| **GPT-3** | üìÑ **Paper:** [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165v4), [News Summarization and Evaluation in the Era of GPT-3](https://arxiv.org/pdf/2209.12356.pdf), [Papers with Code - GPT-3 Explained](https://paperswithcode.com/method/gpt-3)|
| **DANCER** | üìÑ **Paper:** [A Divide-and-Conquer Approach to the Summarization of Long Documents](https://arxiv.org/abs/2004.06190)|

- [Simple Application to summarize data using GPT-3 Openai model](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/simple-application-to-summarize-data-using-gpt-3-openai-model/ba-p/3047978)
- [GPT3-text-summarization](https://github.com/juan-csv/GPT3-text-summarization)
- [Awesome GPT-3](https://github.com/elyase/awesome-gpt3)

### GPT-3: Generative Pre-trained Transformer

- üìÑ **Paper:** [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
  - üìÉ Other related papers: [News Summarization and Evaluation in the Era of GPT-3](https://arxiv.org/abs/2209.12356)
- **Implementations:**
- üì∞ **Articles:**
  - [State of the Art GPT-3 Summarizer For Any Size Document or Format](https://www.width.ai/post/gpt3-summarizer)

#### Zero Shot Text Summarization With GPT-3

Zero shot text summarization refers to using GPT-3 to summarize a given text input without providing any examples in the prompt.

**Large Document Zero Shot Summarization Problems**

You‚Äôll need to split the input text into smaller chunks to pass through GPT-3. Should I just try to fill up as much of the prompt as I can with each run to minimize the total runs? One problem with this is that your model's ability to understand each sentence and its importance to the overall chunk will go down as the size grows. This doesn‚Äôt really affect extractive summarization as much you can simply just increase the sentence count, but abstractive will take a hit as it becomes harder to decide what information is valuable enough to fit into a summary as the ‚Äúpossible information pool‚Äù grows. You also limit the size of your summary that can be generated. 

Smaller chunks allow for more understanding per chunk but increase the risk of split contextual information. Let‚Äôs say you split a dialog or topic in half when chunking to summarize. If the contextual information from that dialog or topic is small or hard to decipher per chunk that model might not include it at all in the summary for either chunk. You‚Äôve now taken an important part of the overall text and split the contextual information about it in half reducing the model's likelihood to consider it important. On the other side you might produce two summaries of the two chunks dominated by that dialog or topic.

**Few Shot Summarization**

Few shot learning with GPT-3 refers to taking the underlying task agnostic large language model and showing the prompt actual examples of how to complete the task. The model combines its trained understanding of how to predict the next token in a language sequence and the ‚Äúpattern‚Äù it picks up on in the prompt through examples to produce a much higher accuracy result. Accuracy is an odd idea here, as it really just follows the examples and tries to fit its quick learning to the new input. As you can imagine, if your examples are incorrect (sentiment analysis) or don't contain the output you would want, you‚Äôll get a result that you don‚Äôt want. 

Few shot learning with relevant examples has been shown to **boost the accuracy of GPT-3 up to 30%** for some tasks, and the boost for summarization is no different. Relevant prompt examples help guide our GPT-3 summarizer to include specific language and topics in our summary without needing overly structured prompt instructions that can lead to overfitting. 

One key difference is our new ability to steer the model towards results that exactly fit what we want without needing multiple steps. We can use our examples to affect both extractive and abstractive type summarizations to account for what we want. 

One of the main differences between the two systems is how we set up the architecture for producing summaries. It‚Äôs no secret that GPT-3 performs better when the examples provided are relevant to the input text. Examples that discuss the same topics, are from the same news article or are closely related help GPT-3 better understand how to map input language to output language for our specific task. 

Most of full scale production summarization architectures are few shot learning based as seen them to produce the most flexibility and highest ‚Äúaccuracy‚Äù towards our goal. 

## Evaluation

- [SummEval: Re-evaluating Summarization Evaluation](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation) by **MIT Press Direct**
- [NLP-progress - Summarization](http://nlpprogress.com/english/summarization.html) - Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.

| Title | Description, Information |
| :---:         |          :--- |
| **ROUGE: A Package for Automatic Evaluation of Summaries** | üìÑ **Paper:** [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013/)|

**ROUGE: A Package for Automatic Evaluation of Summaries**

ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, measures the similarity of two texts by computing n-gram or word sequence overlaps on the scale from 0 to 100. Three variants of ROUGE which are commonly used in summarization includes:

- ROUGE-1: refers to the overlap of unigram (each word) between the two
- ROUGE-2: refers to the overlap of bigram (each two consecutive words) between the two
- ROUGE-L: refers to the longest common sequence between the two



## Articles

- [Bootcamp Tech Blog #4: Long Document Summarization](https://cinnamonai.medium.com/bootcamp-tech-blog-4-long-document-summarization-6bc25e3add94)
