<img src="https://raw.githubusercontent.com/ElizaLo/NLP-Natural-Language-Processing/master/img/Data_Augmentation.png" width="1050" height="150"/>

[**Data augmentation**](https://en.wikipedia.org/wiki/Data_augmentation) in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a [regularizer](https://en.wikipedia.org/wiki/Regularization_(mathematics)) and helps reduce [overfitting](https://en.wikipedia.org/wiki/Overfitting) when training a machine learning model. It is closely related to [oversampling](https://en.wikipedia.org/wiki/Oversampling_and_undersampling_in_data_analysis) in data analysis.

## Lexical Substitution

These approaches try to substitute words present in a text without changing the meaning of the sentence.

## ðŸ“„ Papers

- [Papers with Code - Data Augmentation](https://paperswithcode.com/task/data-augmentation)
- [Papers with Code - Text Augmentation](https://paperswithcode.com/task/text-augmentation)
---
- [Curriculum Data Augmentation for Highly Multiclass Text Classification](https://www.microsoft.com/en-us/research/publication/curriculum-data-augmentation-for-highly-multiclass-text-classification/), 19th Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2021) | March 2021
  > This paper explores data augmentationâ€”a technique particularly suitable for training with limited dataâ€”for highly multiclass text classification tasks, which have a large number of output classes. On four diverse highly multi-class tasks, we find that well-known data augmentation techniques (Sennrich et al., 2016b;Wang et al., 2018; Wei and Zou, 2019) can improve performance by up to 3.0% on average. To further boost performance, we present a simple training strategy called curriculum data augmentation, which leverages curriculum learning by first training on only original examples and then introducing augmented data as training progresses. We explore a two-stage and a gradual schedule, and find that, compared with standard single-stage training, curriculum data augmentation improves performance, trains faster, and maintains robustness high augmentation temperatures (strengths)
- [A Survey of Data Augmentation Approaches for NLP](https://arxiv.org/abs/2105.03075)
  > Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area.
  - :octocat: [Data Augmentation Techniques for NLP](https://github.com/styfeng/DataAug4NLP)

## ðŸ“° Articles

- [Data Augmentation](https://en.wikipedia.org/wiki/Data_augmentation) on Wikipedia
- [A Visual Survey of Data Augmentation in NLP](https://amitness.com/2020/05/data-augmentation-for-nlp/)
- [Data Augmentation in NLP: Best Practices From a Kaggle Master](https://neptune.ai/blog/data-augmentation-nlp) - Collection of papers and resources for data augmentation for NLP.

## :octocat: GitHub Repositories

| Title | Description, Information |
| :---:         |          :--- |
|[Data Augmentation Techniques for NLP](https://github.com/styfeng/DataAug4NLP)|<p>Collection of papers and resources for data augmentation for NLP.</p><p>Papers grouped by _text classification, translation, summarization, question-answering, sequence tagging, parsing, grammatical-error-correction, generation, dialogue, multimodal, mitigating bias, mitigating class imbalance, adversarial examples, compositionality, and automated augmentation._</p>|
