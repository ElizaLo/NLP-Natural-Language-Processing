# Text Summarization

- [Text Summarization](https://paperswithcode.com/task/text-summarization) on **PapersWithCode**
- []()

----
<img src="https://raw.githubusercontent.com/ElizaLo/NLP-Natural-Language-Processing/master/Text%20Summarization/img/ML7062-img1.jpg" width="644" height="233">

- You can apply text summarization to **identify key sentences within a document** or **identify key sentences across multiple documents**.
- Text summarization can produce two types of summaries: **extractive** and **abstractive**. 
  - **Extractive** summaries donâ€™t contain any machine-generated text and are a collection of important sentences selected from the input document. 
  - **Abstractive** summaries contain new human-readable phrases and sentences generated by the text summarization model. Traditional extractive models develop heuristics for scoring textual components by employing surface features (e.g., term frequency, text position, and critical keywords) as well as semantic relationships (e.g., discourse trees) among components in the document. [TextRank](https://www.aclweb.org/anthology/W04-3252/) is a case in point. Alternatively, extractive summarization can be formulated as a binary classification problem in which hand-crafted features are combined to predict whether a fragment should be included in the summary.
  - **Hybrid models:** adopt a two-stage procedure of content selection and paraphrasing. Extractors are employed in content selection to identify important fragments in source documents, which further influence abstractors in generating summaries.

- Most text summarization systems are based on **extractive summarization** because accurate abstractive text summarization is difficult to achieve.

## Datasets

- [Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model](https://arxiv.org/pdf/1906.01749v3.pdf)
  - > While compressing information into a shorter text is the goal of summarization, this dataset tests the ability of abstractive models to generate fluent text concise in meaning while also coherent in the entirety of its generally longer output, which we consider an interesting challenge. 
- 

## Models

| Title | Description, Information |
| :---:         |          :--- |
| **BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - Model Overview** | ðŸ“„ **Paper:** [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461), **abstractive** summarization |
| **SBERT: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks** | ðŸ“„ **Paper:** [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084), **extractive** summarization |

### Extractive summarization

### SBERT: Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks

 In SBERT you can create a summary with the ability to say how many sentences you want. 

- ðŸ“„ **Paper:** [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084)
- **Implementations:**
  - [SentenceTransformers Documentation](https://www.sbert.net/index.html)
  - [bert-extractive-summarizer](https://pypi.org/project/bert-extractive-summarizer/)
  - ðŸ¤— **HuggingFace:** 
    - [sentence-transformers](https://huggingface.co/models?library=sentence-transformers&sort=downloads)

### Abstractive summarization

### BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension - Model Overview

- ðŸ“„ **Paper:** [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)
- :hammer_and_wrench: **Implementations:**
  - ðŸ¤— **HuggingFace:** 
    - [facebook/bart-large-cnn](https://huggingface.co/facebook/bart-large-cnn)
    - [sshleifer/distilbart-cnn-12-6](https://huggingface.co/sshleifer/distilbart-cnn-12-6)
      - ðŸ“„ **Paper:** [Pre-trained Summarization Distillation](https://arxiv.org/abs/2010.13002)
      - > The model has been trained on two datasets: TheÂ [CNN Dailymail dataset](https://huggingface.co/datasets/cnn_dailymail) and theÂ [Extreme Summarization (XSum) dataset](https://huggingface.co/datasets/xsum). The numbers 12 and 6 in the model name refer to the number of encoder layers and decoder layers, respectively.
    - 
-  ðŸ“° **Articles:**
    - [Transformers BART Model Explained for Text Summarization](https://www.projectpro.io/article/transformers-bart-model-explained/553)
- :gear: **Notebooks:** 
    - [DistilBART-CNN-12-6](https://github.com/ElizaLo/NLP-Natural-Language-Processing/blob/master/Text%20Summarization/DistilBART-CNN-12-6_Hugging_Face.ipynb) - Text Summarization with Amazon SageMaker and Hugging Face
    - [facebook_bart-large-cnn.ipynb](https://github.com/ElizaLo/NLP-Natural-Language-Processing/blob/master/Text%20Summarization/facebook_bart-large-cnn.ipynb) - BART (large-sized model), fine-tuned on CNN Daily Mail by Facebook


BART is a transformer encoder-encoder (seq2seq) model that combines an autoregressive (GPT-like) decoder with a bidirectional (BERT-like) encoder.

BART ( Bidirectional and Auto-Regressive) from transformers is a sequence-to-sequence model trained as a denoising autoencoder. This means that a fine-tuned BART model can take a text sequence (for example, English) as input and produce a different text sequence at the output (for example, French).

BART was trained as a denoising autoencoder, so the training data includesÂ _â€œcorruptedâ€_ Â orÂ _â€œnoisyâ€_Â  text, which would be mapped to clean or original text. So what exactly counts asÂ _â€œnoisyâ€_Â  for text data. The authors of BART settle on using some existing and some new noising techniques for pretraining. The noising schemes they use are Token Masking, Token Deletion, Text Infilling, Sentence Permutation, and Document Rotation. 

Looking into each of these transformations:

- **Token Masking:** Random tokens in a sentence are replaced with [MASK]. The model learns how to predict the single token based on the rest of the sequence.
- **Token Deletion:** Random tokens are deleted. The model must learn to predict the token content and find the position where the token was deleted from.
- **Text Infilling:** A fixed number of contiguous tokens are deleted and replaced with a single [MASK] token. The model must learn the content of the missing tokens and the number of tokens.
- **Sentence Permutation:** Sentences (separated by full stops) are permuted randomly. This helps the model to learn the logical entailment of sentences.
- **Document Rotation:** The document is rearranged to start with a random token. The content before the token is appended at the end of the document. This gives insights into how the document is typically arranged and how the beginning or ending of a document looks like.

However, not all transformations are employed in training the final BART model. Based on a comparative study of pre-training objectives, the authors use only text infilling and sentence permutation transformations, with about 30% of tokens being masked and all sentences permuted.

BARTÂ is the _**abstractive model**_ the same as other popular models for text summarization such asÂ **Pegasus**, which means summaries contain new human-readable phrases and sentences generated by the model.

For the summarization task we need to not only stack the inputs but also prepare the targets on the decoder side. BART is an encoder-decoder transformer and thus has the classicÂ seq2seqÂ architecture. In a `seq2seq` setup, a common approach is to apply _â€œteacher forcingâ€_ in the decoder. With this strategy, the decoder receives input tokens (like in decoder-only models such as GPT-2) that consists of the labels shifted by one in addition to the encoder output; so, when making the prediction for the next token the decoder gets the ground truth shifted by one as an input.

**How many Parameters does BART have?**

BART is constructed from a bi-directional encoder like in BERT and an autoregressive decoder like GPT. BERT has around 110M parameters while GPT has 117M, such trainable weights. BART being a sequenced version of the two, fittingly has nearly 140M parameters. Many parameters are justified by the supreme performance it yields on several tasks compared to fine-tuned BERT or its variations like RoBERTa, which has 125M parameters in its base model.

BART outperforms RoBERTa in several fine-tuning tasks.

#### :bangbang: Issues

- [ ] The long text input
  - [Bart now enforces maximum sequence length in Summarization Pipeline #4224](https://github.com/huggingface/transformers/issues/4224)
  - [pipeline does not do truncation on long texts input, error message found #5983](https://github.com/huggingface/transformers/issues/5983)
  
    > We see that the articles can be very long compared to the target summary. **Long articles pose a challenge to most transformer models since the context size is usually limited to 1024 tokens or so**, which is equivalent to a few paragraphs of text. _The standard, yet crude way to deal with this for summarization is to simply truncate the texts beyond the modelâ€™s context size. Obviously there could be important information for the summary toward the end of the text, but for now we need to live with this limitation of the model architectures._

    > **How we can summarize documents where the texts are longer than the modelâ€™s context length?** _Unfortunately, there is no single strategy to solve this problem, and to date this is still an open and active research question._

    > Unfortunately, this problem also manifests when deploying BART on SageMaker via sagemaker.huggingface.HuggingFaceModel. When a request with **> 1024 tokens is sent**, the SageMaker endpoint crashes with an out-of-range CUDA error . What's worse, subsequent requests with smaller inputs fail with the same CUDA error. The only fix is to redeploy the endpoint.
  
    > For now, we're using an encode-truncate-decode workaround like below, but there clearly has to be a better way:
  
```python
# Inputs longer than 1024 tokens cause irrecoverable CUDA errors on
# SageMaker. Make sure that each text is at most 1024 tokens.
inputs = self.tokenizer(texts, max_length=1024, padding="longest",
                        truncation=True)
truncated_texts = [self.tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                   for i in inputs["input_ids"]]
output = predictor.predict({"inputs": truncated_texts, "parameters": parameters})
summaries = [summary["summary_text"] for summary in output]
```

**Possible solutions:**

- Padding and truncation

  - [Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation)
  - [pipeline parameters not read by the deployed endpoint](https://issuehint.com/issue/aws/sagemaker-huggingface-inference-toolkit/37)

## Long Text Summarization

Long documents introduce new problems to the process of summarization:

- **More noise:** It is safe to say that the number of main points within a 5000-word document hardly ever exceeds 10. That is to say, most parts in the document are barely expansions of some central ideas and thus, should be ignored.
- **Scattered main points:** Although there are a few of them, main points are widely scattered over the text, which makes full-text scan inevitable to extract all important information.
- **More resources needed:** As the text gets longer, we need a higher dimensional vector to encode it before feeding into neural models.

Four strategies are proposed to address the issues with long documents:

- **Truncating input to a fixed length.** This is the most straightforward approach but often the least efficient as it discards text which may contain important information, given main points are widely scattered over the text.
- **Focus on informative parts of the document.** We hypothesize that main points should be mentioned in some â€œmainâ€ sections in the document. By that, we only need to perform summarization on the subset of the original text.
  - >  **Shortcomings:** This is basically a heuristic method and cannot scale well if there are no bias sections, for example, in summarizing a novel.
- **Hybrid models.** Extracting only important portions of the text greatly reduces the problem space. This is a promising idea since we can take advantage of state-of-the-art results in both extractive and abstractive text summarization.
  - > **Shortcomings:** Since neural networks are mostly employed in state-of-the-art extractive summarization models, the problem of resources goes full circle.
- **Divide-and-conquer.** In each base case, we try to address the problem of summarizing a text portion within the original document. Then, partial summaries are combined in some way to create the final summary. This method possesses many properties to help us deal with mentioned issues of long documents: (1) by dividing a long text into manageable chunks, we can fully encode them within the constraint of resources; (2) subproblems are independently solved, which makes it easier for parallelism and full-text scan; (3) last but not least, the state-of-the-art results in the field do not go to waste.
  - > **Shortcomings:** Currently, each type of document requires a different dividing strategy. How to efficiently divide an arbitrary text into appropriate sections with relatively short length remains an open question that lacks general answers.


- [An Empirical Survey on Long Document Summarization](https://github.com/huankoh/long-doc-summarization)
- [4 Powerful Long Text Summarization Methods With Real Examples](https://www.width.ai/post/4-long-text-summarization-methods)


The main con we see with long text summarization using **BertSum** is the underlying token limit of **BERT**. **BertSum** has an **_input token limit of 512_** which is much smaller than what we see today with **GPT-3** **_(4000+ in the newest instruct version)_**. This means for long text summarization we have to do a few special things:

1. Build chunking algorithms to split up the text.
2. Manage a new level of data variance considering each text chunk doesnâ€™t contain all contextual information from above. 
3. Manage combining chunk outputs at the end.

### Models

| Title | Description, Information |
| :---:         |          :--- |
| **BertSum** | ðŸ“„ **Paper:** [Fine-tune BERT for Extractive Summarization](https://arxiv.org/abs/1903.10318) - **Extractive Summarization**, [Evaluating Extractive Text Summarization with BERTSUM](https://web.stanford.edu/class/cs224n/reports/final_reports/report042.pdf) |
| **Recursively Summarizing Books with Human Feedback** | ðŸ“„ **Paper:** [Recursively Summarizing Books with Human Feedback](https://arxiv.org/abs/2109.10862)|
|**Longformer Summarization: Long-Document Transformer** | ðŸ“„ **Paper:** [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150v2), [Papers with Code - Longformer: The Long-Document Transformer](https://paperswithcode.com/paper/longformer-the-long-document-transformer#code), [HuggingFace - Longformer](https://huggingface.co/docs/transformers/model_doc/longformer), [Finetune **Longformer Encoder-Decoder (LED)** on 8K Tokens ](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_(LED)_for_Summarization_on_pubmed.ipynb#scrollTo=W7-QHmRiAMB9) |
| **GPT-3** | ðŸ“„ **Paper:** [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165v4), [News Summarization and Evaluation in the Era of GPT-3](https://arxiv.org/pdf/2209.12356.pdf), [Papers with Code - GPT-3 Explained](https://paperswithcode.com/method/gpt-3)|
| **DANCER** | ðŸ“„ **Paper:** [A Divide-and-Conquer Approach to the Summarization of Long Documents](https://arxiv.org/abs/2004.06190)|

- [Simple Application to summarize data using GPT-3 Openai model](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/simple-application-to-summarize-data-using-gpt-3-openai-model/ba-p/3047978)
- [GPT3-text-summarization](https://github.com/juan-csv/GPT3-text-summarization)
- [Awesome GPT-3](https://github.com/elyase/awesome-gpt3)

### GPT-3: Generative Pre-trained Transformer

- ðŸ“„ **Paper:** [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)
  - ðŸ“ƒ Other related papers: [News Summarization and Evaluation in the Era of GPT-3](https://arxiv.org/abs/2209.12356)
  - [Papers with Code - GPT-3 Explained](https://paperswithcode.com/method/gpt-3)
- :hammer_and_wrench: **Implementations:**
  - [OpenAI API - GPT-3 Documentation](https://beta.openai.com/docs/models/gpt-3)
  - [Fine-tuning](https://beta.openai.com/docs/guides/fine-tuning)
- ðŸ“° **Articles:**
  - [State of the Art GPT-3 Summarizer For Any Size Document or Format](https://www.width.ai/post/gpt3-summarizer)
- :gear: **Notebook:** 
  - [GPT-3.ipynb](https://github.com/ElizaLo/NLP-Natural-Language-Processing/blob/master/Text%20Summarization/GPT-3.ipynb) - GPT-3 - Generative Pre-trained Transformer 3, **_model_**: `text-davinci-003` (released: _November 2022_)

#### Zero Shot Text Summarization with GPT-3

Zero shot text summarization refers to using GPT-3 to summarize a given text input without providing any examples in the prompt. We simply provide the instructions for what we want GPT-3 to do and provide the text. 

The GPT-3 playground provides another example of summarization by simply adding a â€œtl;drâ€ to the end of the text passage. They consider this a â€œno instructionâ€ example as they have not specified an initial task and rely entirely on the underlying language models' understanding of what â€œtl;drâ€ means.

**Zero Shot Summarization Explained**

Zero shot based GPT-3 prompts allow you to utilize the underlying model's understanding of your task given through instructions or headers in the prompt without being affected by examples. The model is only steered by any headers and instructions and it leverages these to grow its understanding of what you consider correct. At the end of the day, most GPT-3 tasks are somewhat relative. What you consider a correct response vs what the model considers correct vs what a client considers correct can all be somewhat different. In summarization, it can mean an emphasis on specific keywords, topics, or phrases. It can mean a specific length or contain specific proper nouns. 

Because we havenâ€™t included any real information through examples or a more specific prompt youâ€™re at the mercy of the models underlying understanding. This makes it more difficult to steer the model towards what we might consider a good summary. This is the key flaw in zero shot summarization, as it becomes much harder to fit our prompt to a test dataset as the variation grows. Language changes to just the header and instruction cause less and less change as a few things happen:
1. The size of the input text grows or becomes exponentially shorter. This also depends on the [GPT-3 engine](https://beta.openai.com/docs/engines/gpt-3)
2. The variance in the type or input text or origin of the input text grows. If weâ€™ve fit a nice zero shot summarization model to paragraphs out of a textbook, then move to research papers we will most likely see a drop in accuracy.
â€
This makes zero shot summarizers relatively unstable and very hard to use in production without a full natural language processing pipeline. There are some use cases where it does make sense. 

**Large Document Zero Shot Summarization Problems**

Youâ€™ll need to split the input text into smaller chunks to pass through GPT-3. Should I just try to fill up as much of the prompt as I can with each run to minimize the total runs? One problem with this is that your model's ability to understand each sentence and its importance to the overall chunk will go down as the size grows. This doesnâ€™t really affect extractive summarization as much you can simply just increase the sentence count, but abstractive will take a hit as it becomes harder to decide what information is valuable enough to fit into a summary as the â€œpossible information poolâ€ grows. You also limit the size of your summary that can be generated. 

Smaller chunks allow for more understanding per chunk but increase the risk of split contextual information. Letâ€™s say you split a dialog or topic in half when chunking to summarize. If the contextual information from that dialog or topic is small or hard to decipher per chunk that model might not include it at all in the summary for either chunk. Youâ€™ve now taken an important part of the overall text and split the contextual information about it in half reducing the model's likelihood to consider it important. On the other side you might produce two summaries of the two chunks dominated by that dialog or topic.

#### **Few Shot Summarization**

Few shot learning with GPT-3 refers to taking the underlying task agnostic large language model and showing the prompt actual examples of how to complete the task. The model combines its trained understanding of how to predict the next token in a language sequence and the â€œpatternâ€ it picks up on in the prompt through examples to produce a much higher accuracy result. Accuracy is an odd idea here, as it really just follows the examples and tries to fit its quick learning to the new input. As you can imagine, if your examples are incorrect (sentiment analysis) or don't contain the output you would want, youâ€™ll get a result that you donâ€™t want. 

Few shot learning with relevant examples has been shown to **boost the accuracy of GPT-3 up to 30%** for some tasks, and the boost for summarization is no different. Relevant prompt examples help guide our GPT-3 summarizer to include specific language and topics in our summary without needing overly structured prompt instructions that can lead to overfitting. 

One key difference is our new ability to steer the model towards results that exactly fit what we want without needing multiple steps. We can use our examples to affect both extractive and abstractive type summarizations to account for what we want. 

One of the main differences between the two systems is how we set up the architecture for producing summaries. Itâ€™s no secret that GPT-3 performs better when the examples provided are relevant to the input text. Examples that discuss the same topics, are from the same news article or are closely related help GPT-3 better understand how to map input language to output language for our specific task. 

Most of full scale production summarization architectures are few shot learning based as seen them to produce the most flexibility and highest â€œaccuracyâ€ towards our goal. 

#### GPT-3 Tokenization

- [GPT-3 Online Tokenizer](https://beta.openai.com/tokenizer)

Tokenizer for GPT-3 is the same as GPT-2: ðŸ¤— [OpenAI GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2#gpt2tokenizerfast).

A helpful rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly Â¾ of a word (so 100 tokens ~= 75 words).

The GPT-3 model can take as input from 4,000 to 2,000 tokens (**not confused with words!**). GPT-3 generates ~125-140% tokenks from the input text). 
> The text with 2,000 words approximately has 2,800 tokens.

#### `text-davinci-003`

> OpenAI releases a new language model for GPT-3 trained with human feedback (**November 2022**). It brings numerous improvements, according to OpenAI.

The new GPT-3 model `â€œtext-davinci-003â€` is based on the [InstructGPT](https://the-decoder.com/openai-million-dollar-investment-and-a-new-ai-model/) models introduced by OpenAI earlier this year, which are optimized with human feedback. These models have already shown that AI models trained with RLHF (Reinforcement Learning from Human Feedback) can achieve better results with the same or even lower parameters.

According to OpenAI alignment researcher Jan Leike, `â€œtext-davinci-003â€` is largely equivalent to the InstructGPT models, but is not identical. The new model â€œscores higher on human preference ratings without being fundamentally more capableâ€ than the underlying base model. For fine-tuning, OpenAI required â€œvery little compute and data to align it compared to pretrainingâ€.

Leike points out that the new GPT model still has â€œimportant limitationsâ€ and, for example, sometimes simply makes up things. However, such missteps should now â€œhopefullyâ€ be less frequent and less serious.

`â€œtext-davinci-003â€` can generate **â€œclearer, more engaging, and more compelling contentâ€** and handle **more complex instructions**, according to OpenAI.

`â€œtext-davinci-003â€` can also write longer texts, according to OpenAI. As a result, the language AI can now take on tasks that were previously unfeasible. 

ðŸ“° **Articles:**

- [OpenAIâ€™s latest GPT-3 model generates better and longer texts](https://the-decoder.com/openais-latest-gpt-3-model-generates-better-and-longer-texts/)

#### :thought_balloon: Conclusions

- This model is very sensitive to the input end, which influences the tokenization and consequently affects the summarized text

> **Warning:** Your text ends in a trailing space, which causes worse performance due to how the API splits text into tokens.

- Itâ€™s also worth taking note of the rather high temperature value and shorter max tokens. 
- Each time model generates text of different lengths. Setting up `max_tokens` doesnâ€™t guarantee that text would be this length.
- Words in upper case are divided wrong into the tokens (e.g. FEBRUARY, CUISINE, RESEARCH, MENU, etc.). It is necessary to reduce all words to lowercase or to the form of ordinary sentences, where only the capital letter in the word is large.
- Generally, the best work notes to summary, TL;DR summarization and extracting keywords. Summarize for a 2nd grader creates short, messy, not coherent summarization that does not cover not all key points or does not cover at all.
- **TL;DR summarization:** create well-structured summarization, covering most key points, sometimes short or could be messy.
- **Notes to summary:** generate the longest text and the most coherent, covering almost all key points and keywords.
- **Keywords:** extract the most relevant keywords from the given text, which cover big generalized topics (e.g. alcohol, delivery, dining, upscale, restaurant, drink, menu, order).

## :test_tube: Evaluation

- [SummEval: Re-evaluating Summarization Evaluation](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00373/100686/SummEval-Re-evaluating-Summarization-Evaluation) by **MIT Press Direct**
- [NLP-progress - Summarization](http://nlpprogress.com/english/summarization.html) - Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.

| Title | Description, Information |
| :---:         |          :--- |
| **ROUGE: A Package for Automatic Evaluation of Summaries** | ðŸ“„ **Paper:** [ROUGE: A Package for Automatic Evaluation of Summaries](https://aclanthology.org/W04-1013/)|

**ROUGE: A Package for Automatic Evaluation of Summaries**

ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, measures the similarity of two texts by computing n-gram or word sequence overlaps on the scale from 0 to 100. Three variants of ROUGE which are commonly used in summarization includes:

- ROUGE-1: refers to the overlap of unigram (each word) between the two
- ROUGE-2: refers to the overlap of bigram (each two consecutive words) between the two
- ROUGE-L: refers to the longest common sequence between the two

## :gear: Tools

| Title | Description, Information |
| :---:         |          :--- |
|[Cohere AI](https://docs.cohere.ai)|[Text Summarization](https://docs.cohere.ai/docs/text-summarization-guide) - This article demonstrates a simple way of using Cohere's generation models to summarize text.|

## :octocat: GitHub Repositories

| Title | Description, Information |
| :---:         |          :--- |
|[awesome-text-summarization](https://github.com/icoxfog417/awesome-text-summarization)|The guide to tackle with the Text Summarization|

## :newspaper: Articles

- [Bootcamp Tech Blog #4: Long Document Summarization](https://cinnamonai.medium.com/bootcamp-tech-blog-4-long-document-summarization-6bc25e3add94)
